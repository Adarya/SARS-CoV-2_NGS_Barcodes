{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 32-bit"
  },
  "interpreter": {
   "hash": "3d8bed974a933c244b01d638bd65feedc5a99421d28fe74f6a1ccc3ec62f36aa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import glob\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "from scipy import stats\r\n",
    "from decimal import Decimal\r\n",
    "\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\r\n",
    "from sklearn import datasets, metrics, model_selection\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "from sklearn.metrics import make_scorer\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "matrix = pd.read_csv(\"data/counts_960.csv\")\r\n",
    "\r\n",
    "print(matrix.shape)\r\n",
    "deg_samples = matrix[matrix['RNASE_P'] < 40]\r\n",
    "matrix = matrix[matrix['RNASE_P'] >= 100]\r\n",
    "matrix.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "matrix['tpm_ratio']=matrix[['RNASE_P','E_gene','N1_gene','ORF1a']].sum(axis=1)\r\n",
    "\r\n",
    "for g in ['RNASE_P','E_gene','N1_gene','ORF1a']:\r\n",
    "    matrix[g]=matrix[g]/matrix['tpm_ratio']\r\n",
    "\r\n",
    "matrix.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X=matrix[['E_gene','N1_gene','ORF1a']]\r\n",
    "print(X.shape)\r\n",
    "\r\n",
    "y=matrix.RNA_sample.values\r\n",
    "y.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Real test set - do not run more than once !"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# do not run more than once\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X , y, random_state=7,train_size=0.8, stratify =y) \r\n",
    "np.savetxt(\"data/X_train.csv\", X_train, delimiter=\",\") # upload from folder exsiting files if needed\r\n",
    "np.savetxt(\"data/X_test.csv\", X_test, delimiter = \",\")\r\n",
    "np.savetxt(\"data/y_train.csv\", y_train, delimiter = \",\")\r\n",
    "np.savetxt(\"data/y_test.csv\", y_test, delimiter= \",\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split train into validation set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# take saved train sets\r\n",
    "\r\n",
    "X = np.loadtxt(\"data/X_train.csv\", delimiter=\",\")\r\n",
    "y = np.loadtxt(\"data/y_train.csv\", delimiter=\",\")\r\n",
    "\r\n",
    "print(X.shape, y.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#split into \"test\" (validation) set and the final train set\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X , y, random_state=7,train_size=0.8, stratify = y) \r\n",
    "np.savetxt(\"data/X_train_val.csv\", X_train, delimiter=\",\") # upload from folder exsiting files if needed\r\n",
    "np.savetxt(\"data/X_test_val.csv\", X_test, delimiter = \",\")\r\n",
    "np.savetxt(\"data/y_train_val.csv\", y_train, delimiter = \",\")\r\n",
    "np.savetxt(\"data/y_test_val.csv\", y_test, delimiter= \",\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf=GradientBoostingClassifier(random_state=7, learning_rate= 0.10, subsample=1.0) #default\r\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=10, scoring=\"roc_auc\")\r\n",
    "print(\"Mean AUC: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\r\n",
    "print(\"AUC: {}\".format(scores))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grid search - exhaustive\r\n",
    "\r\n",
    "parameters = {\r\n",
    "    \"loss\":[\"deviance\",\"exponential\"], #default is deviance\r\n",
    "    \"learning_rate\": [0.05, 0.075, 0.1, 0.125],#, 0.15], #default is 0.1\r\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 2, 9), #default is 2\r\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 1, 9), #default is 1\r\n",
    "    \"max_depth\":[1,3,5,8], #default is 3\r\n",
    "    \"max_features\":[\"log2\",\"sqrt\", None], #default is none\r\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"], #default is friedman mse\r\n",
    "    \"subsample\":[0.75, 0.8, 0.85, 0.9, 0.95, 1.0], #default is 1.0\r\n",
    "    \"n_estimators\":[10,50,100,150] #default is 100\r\n",
    "    }\r\n",
    "\r\n",
    "#passing the scoring function in the GridSearchCV\r\n",
    "clf = GridSearchCV(GradientBoostingClassifier(), parameters,scoring=\"roc_auc\",refit=False,cv=10, n_jobs=-1)\r\n",
    "\r\n",
    "clf.fit(X_train, y_train)\r\n",
    "print(clf.best_params_)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# optimized model - choose best parameters as needed\r\n",
    "clf=GradientBoostingClassifier(random_state=7)\r\n",
    "\r\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=10, scoring=\"roc_auc\")\r\n",
    "print(\"Mean AUC: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\r\n",
    "print(\"AUC: {}\".format(scores))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# now fit and test\r\n",
    "\r\n",
    "fixed_model = clf.fit(X_train, y_train)\r\n",
    "\r\n",
    "y_hat= fixed_model.predict(X_test)\r\n",
    "\r\n",
    "print(\"recall: {}\".format(recall_score(y_test, y_hat)))\r\n",
    "print(\"precision: {}\".format(precision_score(y_test, y_hat)))\r\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, y_hat)))\r\n",
    "print(\"Specificity: {}\".format(classification_report(y_test, y_hat, output_dict=True)['0.0']['recall']))\r\n",
    "\r\n",
    "y_hat_325 = (fixed_model.predict_proba(X_test)[:,1] >= 0.325).astype(int)\r\n",
    "pd.concat([pd.DataFrame(X_test), pd.DataFrame(y_test),pd.DataFrame(y_hat_325)],axis=1).to_csv(\"predictions_GBM.csv\")\r\n",
    "\r\n",
    "metrics.plot_roc_curve(clf, X_test, y_test)  # doctest: +SKIP\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf_NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100, ), random_state=1)\r\n",
    "scores_NN = cross_val_score(clf_NN, X_train, y_train, cv=10, scoring=\"roc_auc\")\r\n",
    "print(\"Mean AUC: %0.2f (+/- %0.2f)\" % (scores_NN.mean(), scores_NN.std() * 2))\r\n",
    "print(\"AUC: {}\".format(scores_NN))\r\n",
    "clf_NN.fit(X, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlp_gs = MLPClassifier(random_state=1, max_iter=100)\r\n",
    "parameter_space = {\r\n",
    "    'hidden_layer_sizes': [(10,), (20,), (100,), (100,50), (50,50), (5,10,30),(30,20,10),(100,50,20)],\r\n",
    "    'activation': ['tanh', 'relu'],\r\n",
    "    'solver': ['sgd', 'adam','lbfgs'],\r\n",
    "    'alpha': [0.0001, 0.0001, 0.05],\r\n",
    "    'learning_rate': ['constant','adaptive'],\r\n",
    "}\r\n",
    "\r\n",
    "clf_NN = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\r\n",
    "clf_NN.fit(X_train, y_train) # X is train samples and y is the corresponding labels\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(clf_NN.best_params_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# now fit and test\r\n",
    "\r\n",
    "clf_NN = MLPClassifier(random_state=7, activation= 'relu', alpha= 0.0001, hidden_layer_sizes= (100,20), learning_rate= 'constant', solver= 'lbfgs')\r\n",
    "clf_NN.fit(X_train, y_train)\r\n",
    "\r\n",
    "y_true, y_pred = y_test , clf_NN.predict(X_test)\r\n",
    "\r\n",
    "print(\"recall: {}\".format(recall_score(y_true, y_pred)))\r\n",
    "print(\"precision: {}\".format(precision_score(y_true, y_pred)))\r\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_true, y_pred)))\r\n",
    "print(\"Specificity: {}\\n\".format(classification_report(y_true, y_pred, output_dict=True)['0.0']['recall']))\r\n",
    "pd.concat([pd.DataFrame(X_test), pd.DataFrame(y_pred),pd.DataFrame(y_test)],axis=1).to_csv(\"predictions.csv\")\r\n",
    "print('Results on the test set:')\r\n",
    "print(classification_report(y_true, y_pred))\r\n",
    "\r\n",
    "metrics.plot_roc_curve(clf_NN, X_test, y_true)  # doctest: +SKIP\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# random forest \r\n",
    "\r\n",
    "clf_RF=RandomForestClassifier(random_state=7)\r\n",
    "scores_RF = cross_val_score(clf_RF, X_train, y_train, cv=10, scoring=\"roc_auc\")\r\n",
    "print(\"Mean AUC: %0.2f (+/- %0.2f)\" % (scores_RF.mean(), scores_RF.std() * 2))\r\n",
    "print(\"AUC: {}\".format(scores_RF))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grid search for RF\r\n",
    "\r\n",
    "# A sample parameter\r\n",
    "\r\n",
    "RF_parameters = {'bootstrap': [True, False],\r\n",
    " 'max_depth': [50, None],\r\n",
    " 'max_features': ['auto', 'sqrt'],\r\n",
    " 'min_samples_leaf': [1, 2, 4],\r\n",
    " 'min_samples_split': [1, 2, 5],\r\n",
    " 'n_estimators': [10, 50 , 100, 200]}\r\n",
    "\r\n",
    "#passing the scoring function in the GridSearchCV\r\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(), RF_parameters,scoring=\"roc_auc\",refit=False,cv=5, n_jobs=-1)\r\n",
    "\r\n",
    "clf_RF.fit(X_train, y_train)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# optimized RF model\r\n",
    "clf_RF=RandomForestClassifier(random_state=7)\r\n",
    "scores_RF = cross_val_score(clf_RF, X, y, cv=5, scoring=\"roc_auc\")\r\n",
    "print(\"Mean AUC: %0.2f (+/- %0.2f)\" % (scores_RF.mean(), scores_RF.std() * 2))\r\n",
    "print(\"AUC: {}\".format(scores_RF))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# now fit \r\n",
    "\r\n",
    "fixed_model_RF = clf_RF.fit(X_train, y_train)\r\n",
    "\r\n",
    "y_hat_RF= fixed_model_RF.predict(X_test)\r\n",
    "#y_hat_RF = (clf.predict_proba(X_test)[:,1] >= 0.58).astype(int)\r\n",
    "\r\n",
    "print(\"recall: {}\".format(recall_score(y_test, y_hat_RF)))\r\n",
    "print(\"precision: {}\".format(precision_score(y_test, y_hat_RF)))\r\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, y_hat_RF)))\r\n",
    "print(\"Specificity: {}\\n\".format(classification_report(y_test, y_hat_RF, output_dict=True)['0.0']['recall']))\r\n",
    "\r\n",
    "metrics.plot_roc_curve(clf_RF, X_test, y_test)  # doctest: +SKIP\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fpr, tpr, thresh = metrics.roc_curve(y_test, clf_NN.predict_proba(X_test)[:,1])\r\n",
    "auc = metrics.roc_auc_score(y_test, clf_NN.predict_proba(X_test)[:,1])\r\n",
    "plt.plot(fpr,tpr,label=\"Artificial Neural Network, AUC = \"+str(round(auc, 3)))\r\n",
    "\r\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, clf.predict_proba(X_test)[:,1])\r\n",
    "auc = metrics.roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\r\n",
    "plt.plot(fpr,tpr,label=\"Gradient Boosting Classifier, AUC = \"+str(round(auc, 3)))\r\n",
    "\r\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, clf_RF.predict_proba(X_test)[:,1])\r\n",
    "auc = metrics.roc_auc_score(y_test, clf_RF.predict_proba(X_test)[:,1])\r\n",
    "plt.plot(fpr,tpr,label=\"Random Forest, AUC = \"+str(round(auc, 3)))\r\n",
    "\r\n",
    "\r\n",
    "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\r\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\r\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\r\n",
    "\r\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\r\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\r\n",
    "\r\n",
    "plt.plot([0,1], [0,1], color='gray', linestyle='--')\r\n",
    "\r\n",
    "plt.legend(loc=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Thresholds"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def frange(start, stop, step=1.0):\r\n",
    "    ''' \"range()\" like function which accept float type''' \r\n",
    "    i = start\r\n",
    "    while i < stop:\r\n",
    "        yield i\r\n",
    "        i += step"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "threshold_table = pd.DataFrame(columns=['Model', 'threshold', 'Accuracy', 'Precision', 'Sensitivity'])\r\n",
    "\r\n",
    "for th in frange(0.25, 0.6, 0.025):\r\n",
    "    y_hat = (clf.predict_proba(X_test)[:,1] >= th).astype(int)\r\n",
    "    y_hat_NN = (clf_NN.predict_proba(X_test)[:,1] >= th).astype(int)\r\n",
    "    report = classification_report(y_test, y_hat, output_dict=True)\r\n",
    "    report_NN = classification_report(y_test, y_hat_NN, output_dict=True)\r\n",
    "\r\n",
    "    threshold_table = threshold_table.append({'Model':'Artificial Neural Network', \r\n",
    "                                        'threshold': th,                           \r\n",
    "                                        'Precision':precision_score(y_test, y_hat_NN),\r\n",
    "                                        'Accuracy':accuracy_score(y_test, y_hat_NN), \r\n",
    "                                        'Sensitivity':recall_score(y_test, y_hat_NN),\r\n",
    "                                        'Specificity': report['0.0']['recall']}, ignore_index=True)\r\n",
    "\r\n",
    "    threshold_table = threshold_table.append({'Model':'Gradient Boosting Classifier', \r\n",
    "                                        'threshold': th,                           \r\n",
    "                                        'Precision':precision_score(y_test, y_hat),\r\n",
    "                                        'Accuracy':accuracy_score(y_test, y_hat), \r\n",
    "                                        'Sensitivity':recall_score(y_test, y_hat),\r\n",
    "                                        'Specificity': report['0.0']['recall']}, ignore_index=True)\r\n",
    "\r\n",
    "threshold_table.set_index('Model').sort_values(['Model','threshold'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\r\n",
    "\r\n",
    "filename = 'GBM_model_val_noRNASE.sav'\r\n",
    "pickle.dump(clf, open(filename, 'wb'))\r\n",
    "\r\n",
    "filename = 'ANN_model_val_noRNASE.sav'\r\n",
    "pickle.dump(clf_NN, open(filename, 'wb'))\r\n",
    "\r\n",
    "filename = 'RF_model_val_noRNASE.sav'\r\n",
    "pickle.dump(clf_RF, open(filename, 'wb'))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import joblib\r\n",
    "\r\n",
    "filename = 'GBM_model_val_rnaseInc.joblib'\r\n",
    "joblib.dump(clf, filename)\r\n",
    "\r\n",
    "filename = 'ANN_model_val_rnaseInc.joblib'\r\n",
    "joblib.dump(clf_NN, filename)\r\n",
    "\r\n",
    "filename = 'RF_model_val_rnaseInc.joblib'\r\n",
    "joblib.dump(clf_RF, filename)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}